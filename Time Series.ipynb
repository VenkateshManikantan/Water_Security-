{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b008f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dateformat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Statsmodels\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data = pd.read_csv(r'WATER_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop GLDAS vars\n",
    "water_data.columns.str.contains('_', case=False)\n",
    "water_data_nogldas = water_data.loc[:,~water_data.columns.str.contains('_', case=False)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c50b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to specific location, temporarily specified specific lon/lan since data is not yet by country\n",
    "df= water_data_nogldas[(water_data_nogldas.lon == -179.5) & (water_data_nogldas.lat == 67.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping not needed variables, Year is index\n",
    "df = df.drop(['A', 'lon', 'lat'], axis=1).set_index('year')\n",
    "print(df.columns)\n",
    "print(df.shape) \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Time Series\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=14, ncols=3, dpi=120, figsize=(14,98))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    data = df[df.columns[i]]\n",
    "    ax.plot(data, color='red', linewidth=1)\n",
    "    # Decorations\n",
    "    ax.set_title(df.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219da04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Causation\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=11\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(df, variables = df.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6db92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cointegration Test\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Series into Training and Testing Data\n",
    "nobs = 4\n",
    "df_train, df_test = df[0:-nobs], df[-nobs:]\n",
    "\n",
    "# Check size\n",
    "print(df_train.shape)  # (33, 42)\n",
    "print(df_test.shape)  # (4, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068de77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Stationarity and Make the Time Series Stationary\n",
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key,val in r[4].items():\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\") \n",
    "        \n",
    "# ADF Test on each column\n",
    "for name, column in df_train.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c71bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Difference\n",
    "# 2nd Difference\n",
    "# Until Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca559aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Select the Order (P) of VAR model\n",
    "model = VAR(df_differenced)\n",
    "for i in [1,2,3,4,5,6,7,8,9]:\n",
    "    result = model.fit(i)\n",
    "    print('Lag Order =', i)\n",
    "    print('AIC : ', result.aic)\n",
    "    print('BIC : ', result.bic)\n",
    "    print('FPE : ', result.fpe)\n",
    "    print('HQIC: ', result.hqic, '\\n')\n",
    "\n",
    "x = model.select_order(maxlags=12)\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAR Model of Selected Order(p) --- p depends above\n",
    "model_fitted = model.fit(4)\n",
    "model_fitted.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88745e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(adjust(col), ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast VAR model using statsmodels\n",
    "# Get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order)  #> 4\n",
    "\n",
    "# Input data for forecasting\n",
    "forecast_input = df_differenced.values[-lag_order:]\n",
    "forecast_input\n",
    "\n",
    "\n",
    "# Forecast\n",
    "fc = model_fitted.forecast(y=forecast_input, steps=nobs)\n",
    "df_forecast = pd.DataFrame(fc, index=df.index[-nobs:], columns=df.columns + '_2d')\n",
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76816696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the transformation to get the real forecast\n",
    "def invert_transformation(df_train, df_forecast, second_diff=False):\n",
    "    \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n",
    "    df_fc = df_forecast.copy()\n",
    "    columns = df_train.columns\n",
    "    for col in columns:        \n",
    "        # Roll back 2nd Diff\n",
    "        if second_diff:\n",
    "            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n",
    "        # Roll back 1st Diff\n",
    "        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()\n",
    "    return df_fc\n",
    "\n",
    "df_results = invert_transformation(train, df_forecast, second_diff=True)        \n",
    "# Should specify our variables: sample only below\n",
    "#df_results.loc[:, ['rgnp_forecast', 'pgnp_forecast', 'ulc_forecast', 'gdfco_forecast',\n",
    "                   'gdf_forecast', 'gdfim_forecast', 'gdfcf_forecast', 'gdfce_forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of Forecast vs Actuals\n",
    "fig, axes = plt.subplots(nrows=int(len(df.columns)/2), ncols=2, dpi=150, figsize=(10,10))\n",
    "for i, (col,ax) in enumerate(zip(df.columns, axes.flatten())):\n",
    "    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    df_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79179ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Forecasts\n",
    "from statsmodels.tsa.stattools import acf\n",
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})\n",
    "\n",
    "# Need to call for each variable: sample only below\n",
    "print('Forecast Accuracy of: rgnp')\n",
    "accuracy_prod = forecast_accuracy(df_results['rgnp_forecast'].values, df_test['rgnp'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
